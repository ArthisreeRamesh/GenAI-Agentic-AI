# -*- coding: utf-8 -*-
"""Aug3EndTevvaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CSjJ80crzhuSQqdY6KIcJweAF09Topmn
"""

!pip install langchain
!pip install langchain-openai
!pip install langchain-community
!pip install langchain-chroma
!pip install ragas
!pip install deepeval
!pip install dill

from getpass import getpass
OPEN_API_KEY=getpass("Enter api key")

import os
os.environ['OPENAI_API_KEY'] = OPEN_API_KEY

from langchain_openai import OpenAIEmbeddings
embeddingmodel= OpenAIEmbeddings(model="text-embedding-3-small")

#this is to download the data but we have it handy so directly uploading
#!gdown 1QkSY9W5RyaBnY8c5FLIsmpPVXoHTQ-fb

import pandas as pd

df= pd.read_csv("/content/sample_data/rag_eval_docs.csv")
df

"""orient	   Structure               produced	Example output
'records'	List of row dicts	       [{'col1': val1, 'col2': val2}, ...]
'dict'	  Dict of columns	         {'col1': [..], 'col2': [..]}
'list'	  Dict of columns as lists {'col1': [...], 'col2': [...]}
'index'	  Dict of index to dicts	 {0: {'col1': val1, ...}, 1: {...}}

The orient parameter controls how the DataFrame is converted to a dictionary.
"""

docs = df.to_dict(orient='records')
docs

#convert to metadata(which is Langchain format to proceed with further processing)
from langchain.docstore.document import Document
precessed_docs = []

for doc in docs:
  metadata = {
      "title": doc['title'],
      "id": doc['id'],
  }
  data = doc['context']
  precessed_docs.append(Document(page_content=data, metadata=metadata))

"""this whol process of csv -> dict -> metadata could be done by Langchain csv loader directly"""

precessed_docs

"""Index Document and Embedding in vector db"""

from langchain_chroma import Chroma
chroma_db= Chroma.from_documents(documents=precessed_docs,embedding =embeddingmodel,collection_name='my_eval',collection_metadata={"hnsw:space":'cosine'},
                                 persist_directory='/myevaldb')

#load from disc
chroma_db =  Chroma(embedding_function =embeddingmodel,collection_name='my_eval',
                                 persist_directory='/myevaldb')

chroma_db

"""#semaintic similarity based retrieval"""

#score_threshold- if similarity score is less than score_threshold it wont be considered as relavant document, on top of this top N documents will be filtered based on k=N

similarity_retriever= chroma_db.as_retriever(search_type="similarity_score_threshold",search_kwargs={ "score_threshold" : 0.3 , "k" : 3}
                                             )

#just to view the retrieved documents in a good format
from IPython.display import display, Markdown

def display_docs(docs):
  for doc in docs:
    print("Metadata :", doc.metadata)
    print("Content Brief:")
    display(Markdown(doc.page_content))
    print()

query='what is AI'
top_docs = similarity_retriever.invoke(query)
display_docs(top_docs)

"""here only one doucment is retrieved though k=3 because only 1 doc in the database is related to photosynthesis"""

query='what is Photosynthesis'
top_docs = similarity_retriever.invoke(query)
display_docs(top_docs)

"""RAG pipeline"""

from langchain_core.prompts import ChatPromptTemplate
rag_prompt = """You are an assistant who is an expert in question-answering tasks.
              Answer the following question using only the following pieces of retrieved context.
              If the answer is not in the context, do not make up answers, just say that you don't know.
              Keep the answer to the points based on the information from the context.

              Question: {question}
              Context: {context}

              Answer:


              """
rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)

"""similarity_retriever (Team 1) → Fetches relevant docs.

src_rag_response_chain (Team 2) → Formats docs + prompts GPT + parses output.

Each chain does its own job well.
If you merged them into one huge function, debugging, replacing parts, or improving performance (like caching retrieval results) becomes messy.

So having two stages = modularity + maintainability.
"""

from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

chatgpt = ChatOpenAI(model="gpt-4o", temperature=0)

def format_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)

src_rag_response_chain = ({
    "context":(itemgetter('context')
    | RunnableLambda(format_docs)),
               "question":itemgetter("question")}
               | rag_prompt_template
               | chatgpt
               | StrOutputParser() )

rag_chain_w_sources = (
    {"context": similarity_retriever, "question": RunnablePassthrough()}
    | RunnablePassthrough.assign(
        response=src_rag_response_chain,
    )
)

query = "how do plants survive?"
result = rag_chain_w_sources.invoke(query)
result

query = "What is Artificial Intelligence?"
result = rag_chain_w_sources.invoke(query)
result

"""evaluation

"""

retrieved_context = [doc.page_content for doc in result['context']]
retrieved_context

human_answer = """AI, also known as Artificial Intelligence is used to build complex systems for applications
                like virtual assistants, robotics and autonomous vehicles."""

#just for us to understand experiment purpose
new_context =["Machine Learning is the study of algorithm which learn with more data",
              "AI is known as Artificial Intelligence "] + retrieved_context

"""evaluation metrics"""

from deepeval.test_case import LLMTestCase
from deepeval.metrics import ContextualPrecisionMetric
from deepeval import evaluate

test_case = LLMTestCase(
    input = result['question'],
    actual_output=result['response'],
    expected_output=human_answer,
    retrieval_context=retrieved_context
)

metrics = ContextualPrecisionMetric(
    threshold=0.55,
    model ='gpt-4o',
    include_reason=True,
    verbose_mode=True)

result = evaluate([test_case], [metrics])

from deepeval.test_case import LLMTestCase
from deepeval.metrics import ContextualPrecisionMetric
from deepeval import evaluate

test_case = LLMTestCase(
    input = result['question'],
    actual_output=result['response'],
    expected_output=human_answer,
    retrieval_context=new_context
)

metrics = ContextualPrecisionMetric(
    threshold=0.55,
    model ='gpt-4o',
    include_reason=True,
    verbose_mode=True)

result = evaluate([test_case], [metrics])

print("Success:", result.test_results[0].metrics_data[0].success)
print("Score:", result.test_results[0].metrics_data[0].score)
print("Reason:", result.test_results[0].metrics_data[0].reason)

new_context1 =["NVIDIA makes chips for AI",
              "AI is known as Artificial Intelligence ",
              "Physics and Maths are simple",
              "Chemistry is really simple if you know the fundamentals"] + retrieved_context

human_answer = """AI, also known as Artificial Intelligence is used to build complex systems for applications
                like virtual assistants, robotics and autonomous vehicles."""

from deepeval.test_case import LLMTestCase
from deepeval.metrics import ContextualRecallMetric
from deepeval import evaluate

test_case = LLMTestCase(
    input = result['question'],
    actual_output=result['response'],
    expected_output=human_answer,
    retrieval_context=new_context1
)

metrics = ContextualRecallMetric(
    threshold=0.55,
    model ='gpt-4o',
    include_reason=True,
    verbose_mode=True)

result = evaluate([test_case], [metrics])

"""in above case (recall) we are getting 100 percent accuracy even though there are bit non relevant documents because false negative is 0. and formula is TP/(FN+TP)"""

from deepeval.test_case import LLMTestCase
from deepeval.metrics import ContextualRelevancyMetric
from deepeval import evaluate

test_case = LLMTestCase(
    input = result['question'],
    actual_output=result['response'],
    expected_output=human_answer,
    retrieval_context=new_context
)

metrics = ContextualRelevancyMetric(
    threshold=0.55,
    model ='gpt-4o',
    include_reason=True,
    verbose_mode=True)

result = evaluate([test_case], [metrics])

"""Generator metrics
since its the chcekpoint in generation part of rag pipeline in test case no need to give context similar to retrieval metrics
"""

from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric
from deepeval import evaluate

test_case = LLMTestCase(
    input = result['question'],
    actual_output=result['response'],
)

metrics = AnswerRelevancyMetric(
    threshold=0.55,
    model ='gpt-4o',
    include_reason=True,
    verbose_mode=True)

result = evaluate([test_case], [metrics])

from deepeval.test_case import LLMTestCase
from deepeval.metrics import FaithfulnessMetric, HallucinationMetric, GEval
from deepeval import evaluate

test_case = LLMTestCase(
    input = result['question'],
    actual_output=result['response'],
    retrieval_context=new_context
)

metrics = FaithfulnessMetric(
    threshold=0.55,
    model ='gpt-4o',
    include_reason=True,
    verbose_mode=True)

result = evaluate([test_case], [metrics])

from deepeval.test_case import LLMTestCase
from deepeval.metrics import FaithfulnessMetric, HallucinationMetric, GEval
from deepeval import evaluate

test_case = LLMTestCase(
    input = result['question'],
    actual_output=result['response'],
    context=new_context1
)

metrics = HallucinationMetric(
    threshold=0.55,
    model ='gpt-4o',
    include_reason=True,
    verbose_mode=True)

result = evaluate([test_case], [metrics])

"""no verbose mode/no reason paramter allowed in Geval

Geval customized evaluation metrix (checks quality)

It mainly depends on these 5 criterias:

Relevance -	Alignment with user question	(Semantic similarity, cosine distance, attention alignment)
Coherence	-Logical flow between sentences	(Discourse modeling, attention dependency, perplexity)
Conciseness	-Minimal but sufficient length	(Information density, MDL principle)
Fluency -	Grammatical and natural phrasing	(LM probability, syntax modeling)
Helpfulness	-Practical usefulness of response	(RLHF reward models, utility optimization)

"""

from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from deepeval import evaluate

# Define your test case
test_case = LLMTestCase(
    input=result['question'],
    actual_output=result['response'],
    context=new_context1
)

# Define GEval metric for relevance
geval_metric = GEval(
    name="Relevance Evaluation",
    criteria="The response should be relevant to the input question and information provided in the context.",
    evaluation_params=[
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.CONTEXT
    ],
    threshold=0.6,
    model="gpt-4o-mini"
)

# Run the evaluation
result = evaluate([test_case], [geval_metric])

# Print results
print(result)