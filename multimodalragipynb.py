# -*- coding: utf-8 -*-
"""MultimodalRAGipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBwKKTm2OoSwd2e6-01GxY875WgwtAm-
"""

!pip install langchain
!pip install langchain-openai
!pip install langchain-chroma
!pip install langchain-community
!pip install redis
!pip install "unstructured[all-docs]"
!pip install htmltabletomd

# to prevent 403 errors with unstructured.io till they fix it
#ideally do this before unstructructured installation
# refer: https://github.com/Unstructured-IO/unstructured/issues/3795
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')

!sudo apt-get install tesseract-ocr #hand written scan or pdf image / scan copy
!sudo apt-get install poppler-utils # unstructured format of PDF, easily convert image into pdf and then image to text

"""PARTITION PDF TABLES,TEXT,IMAGE"""

#!pip install -U "unstructured[all-docs]"==0.16.11

"""this dint work which caused error page empty page content not found so uploaded directly"""

#!wget https://sgp.fas.org/crs/misc/IF10244.pdf

!pip install htmltabletomd==1.0.0

#IF THERE IS ANY FIGURE/IMAGE FILE REMOVE IT ,But right now we have only pdf (remove references)
!rm -rf /content/figures

from langchain_community.document_loaders.pdf import UnstructuredPDFLoader
doc = '/content/sample_data/transformer_paper (1).pdf'
loader = UnstructuredPDFLoader(file_path=doc, strategy='hi_res', extract_images_in_pdf=True,
                               infer_table_structure=True, mode='elements',
                               image_output_dir_path='/content/figures')
data = loader.load()

len(data)

[doc.metadata['category'] for doc in data]

data[0].page_content

[doc.metadata['category'] for doc in data if doc.metadata['category']=='Table']

table_indices = [i for i, doc in enumerate(data) if doc.metadata['category'] == 'Table']
print(table_indices)

tables=[doc for doc in data if doc.metadata['category']=='Table']

tables

"""NOTE:Only if we have text_as_html we ca start statistical analysis"""

from IPython.display import Markdown

display(Markdown(data[92].metadata["text_as_html"]))

"""Since unstructured extracts the text from the table without any borders, we can use the HTML text and put it directly in prompts (LLMs understand HTML tables well) or even better convert HTML tables to Markdown tables as below"""

#just to check how convertion works
import htmltabletomd
md_table = htmltabletomd.convert_table(data[92].metadata['text_as_html'])
print(md_table)

"""seperate data into text and table"""

docs = []
tables = []
for doc in data:
    if doc.metadata['category']=='Table':
        tables.append(doc)
    elif doc.metadata['category']=='NarrativeText':
        docs.append(doc)
    elif doc.metadata['category']=='Title':
        docs.append(doc)
    elif doc.metadata['category']=='UncategorizedText':
        docs.append(doc)
    elif doc.metadata['category']=='Header':
        docs.append(doc)
    elif doc.metadata['category']=='FigureCaption':
        docs.append(doc)

print(len(docs))
print(len(tables))

## Convert HTML tables to Markdown
for table in tables:
  table.page_content = htmltabletomd.convert_table(table.metadata['text_as_html'])

for table in tables:
  print(table.page_content)
  print()

"""#view extracted images"""

!ls -l /content/figures

from IPython.display import Image

#1st page 1st image
Image('/content/figures/figure-3-1.jpg')

"""api key for LLM model

"""

from getpass import getpass
OPEN_API_KEY = getpass("Enter openai API key:")

"""Load Connection to LLM
Here we create a connection to ChatGPT to use later in our chains
"""

from langchain_openai import ChatOpenAI
chatgpt = ChatOpenAI(api_key=OPEN_API_KEY,model='gpt-4o-mini',temperature=0)

"""Text and Table summaries
We will use GPT-4o to produce table and, text summaries.

Text summaries are advised if using large chunk sizes (e.g., as set above, we use 4k token chunks).

Summaries are used to retrieve raw tables and / or raw chunks of text.text and table processing
"""

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough

# Prompt
prompt_text = """
You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.
These summaries will be embedded and used to retrieve the raw text or table elements.
Give a detailed summary of the table or text below that is well optimized for retrieval.
For any tabels also add in a one line description of what the table is about besides the summary.
Do not add redundant words like summary.
just output the actual summary content.

Table or Text chunk:
{element}

"""

prompt = ChatPromptTemplate.from_template(prompt_text)

"""chain"""

summarize_chain = ({"element":RunnablePassthrough()} | prompt | chatgpt | StrOutputParser())
# stroutputparser extracts the response as text and returns it as a string

# initialize empty summaries
text_summaries = []
table_summaries = []

text_docs = [doc.page_content for doc in docs]
table_docs = [table.page_content for table in tables]

text_summaries = summarize_chain.batch(text_docs, {"max_concurrency":5})#concurrency-how many iterations or how many times a paragh is sumarized
table_summaries = summarize_chain.batch(table_docs, {"max_concurrency":5})

print(len(text_summaries))
print(len(table_summaries))

text_summaries[1]

table_summaries[1]

data[1].metadata['category']

data[10]

text_summaries[7]

data[12]

#data[12] is the first table i.e,table_Summary[0] is summary of dirst table
table_summaries[0]

"""Image summaries:
We will use GPT-4o to produce the image summaries.

We pass base64 encoded images
"""

import os
import base64

import os
import base64

from langchain_core.messages import HumanMessage

def encode_image(image_path):
  """Getting the base64 string"""
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

def image_summarize(img_base64, prompt):
  """Make image summary"""
  chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0, api_key = OPEN_API_KEY)
  msg = chat.invoke(
      [
          HumanMessage(content=[{"type":"text", "text":prompt},
                                {"type":"image_url",
                                 "image_url": {"url":f"data:image/jpeg;base64,{img_base64}"}}])
      ]
  )
  return msg.content

def generate_img_summaries(path):
  """
  Generate summaries and base64 encoded strings for images path:
  Path to list of .jpg files extracted by UnstructuredPDFLoader
  """
  # store base64 encoded image
  img_base64_list = []
  # store summaries
  img_summaries = []
  # prompt
  prompt = """
  You are an assistant tasked with summarizing images for retrieval.
  Remember these images could potentially contain graph, charts or tables also.
  These summaries will be embedded and used to retrieve the raw image for question answering.
  Give a detailed summary of the image that is well optimized for retrieval.
  Do not add additional words like summary: etc.
  """

  # Apply to image
  for img_file in sorted(os.listdir(path)):
    if img_file.endswith('.jpg'):
      img_path = os.path.join(path, img_file)
      base64_image = encode_image(img_path)
      img_base64_list.append(base64_image)
      img_summaries.append(image_summarize(base64_image, prompt))
  return img_base64_list, img_summaries

# Image summary -multiple assignment
IMG_PATH = "/content/figures"
imgs_base64, img_summaries = generate_img_summaries(IMG_PATH)

""""data:image/jpeg;base64,..." is a special way of saying â€œthis is an image hidden inside text.â€
| File type  | Official MIME type | Common file extension |
| ---------- | ------------------ | --------------------- |
| JPEG image | `image/jpeg`       | `.jpg` or `.jpeg`     |
| PNG image  | `image/png`        | `.png`                |
| GIF image  | `image/gif`        | `.gif`                |
| PDF file   | `application/pdf`  | `.pdf`                |

So this loop = â€œGo through every image in the folder, turn it into text, and get the AIâ€™s summary.â€
os.listdir(path) â†’ lists all files in that folder.

sorted(...) â†’ arranges them in order (A to Z).

if img_file.endswith('.jpg'): â†’ checks if itâ€™s a picture file.

os.path.join(path, img_file) â†’ builds the full file path (folder + filename).

encode_image(img_path) â†’ turns the image into base64 text.

img_base64_list.append(...) â†’ saves the base64 code.

image_summarize(...) â†’ asks the AI to describe the picture.

img_summaries.append(...) â†’ saves that description.
"""

len(imgs_base64), len(img_summaries)

imgs_base64[1]

display(Image('./figures/figure-4-2.jpg'))

display(Markdown(img_summaries[1]))

"""Vector storage
Multi-vector retriever
Use multi-vector-retriever to index image (and / or text, table) summaries, but retrieve raw images (along with raw texts or tables).

Download and Install Redis as a DocStore
You can use any other database or cache as a docstore to store the raw text, table and image elements

#%%sh run whole shell
when ever we use redis we must do this
redis is kind of temporary memory
we can use just chroma also but in most rags just using chroma will create issues while image data being used etc,redis works perfect for multimodal RAG
"""

# Commented out IPython magic to ensure Python compatibility.
# %%sh
# curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg
# echo "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/redis.list
# sudo apt-get update  > /dev/null 2>&1
# sudo apt-get install redis-stack-server  > /dev/null 2>&1
# redis-stack-server --daemonize yes

"""Open ai embedding models
LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient text-embedding-3-small model, and a larger and more powerful text-embedding-3-large model.
"""

from langchain_openai import OpenAIEmbeddings

# details here: https://openai.com/blog/new-embedding-models-and-api-updates
openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small',api_key=OPEN_API_KEY)

docs[0]

"""Add to vectorstore & docstore
Add raw docs and doc summaries to Multi Vector Retriever:

Store the raw texts, tables, and images in the docstore (here we are using Redis).
Store the texts, table summaries, and image summaries and their corresponding embeddings in the vectorstore (here we are using Chroma) for efficient semantic retrieval.
Connect them using a common document_id
"""

import uuid

from langchain_community.storage import RedisStore
print("redis done")
from langchain_community.utilities.redis import get_client
print("get client done")
from langchain_chroma import Chroma
print("chroma done")
from langchain_core.documents import Document
print("document done")
from langchain_openai import OpenAIEmbeddings
print("embeddings done")
#from langchain.retrievers.multi_vector import MultiVectorRetriever
#from langchain_community.retrievers.multi_vector import MultiVectorRetriever
#from langchain.retrievers.multi_vector import MultiVectorRetriever



print("âœ… All imports succeeded!")

import pkgutil
import langchain
print([m.name for m in pkgutil.walk_packages(langchain.__path__)])

pip install -U langchain langchain-community

"""docstore = the bookshelf with full books & pictures ðŸ“šðŸ–¼ï¸

vectorstore = the cards with short summaries ðŸ—‚ï¸

retriever = the robot helper ðŸ¤– that knows how to find things

add_documents = the robotâ€™s assistant that organizes cards and books properly

id_key = "doc_id"
Every document (story, table, image) will get a special ID called doc_id.so that we can easily link summary to original content
The word enumerate just means:
â€œGive me both the position (number) and the value from the list.â€
page_content = s â†’ the text of the summary.

metadata = {id_key: doc_ids[i]} â†’ a label (like â€œdoc_id: A1â€) that tells which story it belongs to.

So for each summary, we make a Document like:

Document(page_content="Magic wand story", metadata={"doc_id": "A1"})
zip():Itâ€™s like tying two lists together with a rubber band ðŸ”—
so that the first thing from each list goes together,
then the second, then the third, and so on.
set():
Creating doc_ids just makes the names of the drawers(creates ids),
but .set() actually puts the stories inside those drawers.
"""

def create_multi_vector_retriever(
    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images
):
    """
    Create retriever that indexes summaries, but returns raw images or texts
    """


    id_key = "doc_id" #every time user gives it it produce unique id(its used as key in the input of Document for summary docs)


        # Use vectorstore.as_retriever instead of MultiVectorRetriever
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

    # Helper function to add documents to the vectorstore and docstore
    def add_documents(doc_summaries, doc_contents):
        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
        summary_docs = [
            Document(page_content=s, metadata={id_key: doc_ids[i]})
            for i, s in enumerate(doc_summaries)
        ]
        vectorstore.add_documents(summary_docs)
        # Use mset method for RedisStore as it expects multiple key-value pairs
        doc_pairs = [(doc_id, doc_content) for doc_id, doc_content in zip(doc_ids, doc_contents)]
        if doc_pairs:
            docstore.mset(doc_pairs)


    # Add texts, tables, and images
    # Check that text_summaries is not empty before adding
    if text_summaries:
        add_documents(text_summaries, texts)
    # Check that table_summaries is not empty before adding
    if table_summaries:
        add_documents(table_summaries, tables)
    # Check that image_summaries is not empty before adding
    if image_summaries:
        add_documents(image_summaries, images)

    return retriever


# The vectorstore to use to index the summaries and their embeddings
chroma_db = Chroma(
    collection_name="mm_rag",
    embedding_function=openai_embed_model,
    collection_metadata={"hnsw:space": "cosine"},
)

# Initialize the storage layer - to store raw images, text and tables
client = get_client('redis://localhost:6379')
redis_store = RedisStore(client=client) # you can use filestore, memorystory, any other DB store also

# Create retriever
#redis_store,full document
#chroma_db,summaries are stored
retriever_multi_vector = create_multi_vector_retriever(
    redis_store,
    chroma_db,
    text_summaries,
    text_docs,
    table_summaries,
    table_docs,
    img_summaries,
    imgs_base64,
)

"""check retrieval"""

query = "Find the story about magic wand"
summary_results = retriever_multi_vector.invoke(query)

# Get full content
full_docs = [redis_store.mget([doc.metadata["doc_id"]])[0] for doc in summary_results]

"""Storage	Whatâ€™s inside	Role
RedisStore	-Full text, tables, images	Bookshelf with complete content
Chroma DB	-Summaries + embeddings + doc_id	Card catalog for searching summaries

the below chunk is optional just to check images as well

What this code chunk does
Many times, in your project, images are stored as base64 strings in your database (like Redis) or in Chroma metadata.
Base64 is a text version of an image â€” good for storage and transfer, but you cannot see it directly.

The function:
plt_img_base64(img_base64)
Takes the base64 string
Converts it back to a normal image
Displays it in Jupyter or Colab
So now you can actually look at the image, not just its text code.

How it helps in analysis

Verification: Confirm that your retrieval system fetched the right images. âœ…

Exploration: Quickly look at multiple images for a query and compare them. ðŸ”

Presentation: Makes it easy to show results in reports, notebooks, or dashboards. ðŸ“ŠðŸ–¼ï¸
"""

from IPython.display import HTML, display, Image
from PIL import Image
import base64
from io import BytesIO

def plt_img_base64(img_base64):
    """Disply base64 encoded string as image"""
    # Decode the base64 string
    img_data = base64.b64decode(img_base64)
    # Create a BytesIO object
    img_buffer = BytesIO(img_data)
    # Open the image using PIL
    img = Image.open(img_buffer)
    display(img)

"""Examine retrieval; we get back images and tables also that are relevant to our question."""

query = "Analyze the wildfires trend with acres burned over the years"
docs = retriever_multi_vector.invoke(query)

# We get 3 docs
len(docs)

docs

"""Utilities to separate retrieved elements:
We need to bin the retrieved doc(s) into the correct parts of the GPT-4o prompt template.

Here we need to have text, table elements as one set of inputs and image elements as the other set of inputs as both require separate prompts in GPT-4o.
"""

import re
import base64
from operator import itemgetter
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.messages import HumanMessage

def looks_like_base64(sb):
    """Check if the string looks like base64"""
    return re.match("^[A-Za-z0-9+/]+[=]{0,2}$", sb) is not None


def is_image_data(b64data_str):
    """
    Check if the base64 data string is an image by looking at the start of the data
    """
    image_signatures = {
        b"\xff\xd8\xff": "jpg",
        b"\x89\x50\x4e\x47\x0d\x0a\x1a\x0a": "png",
        b"\x47\x49\x46\x38": "gif",
        b"\x52\x49\x46\x46": "webp",
    }
    try:
        header = base64.b64decode(b64data_str)[:8]
        for sig, format in image_signatures.items():
            if header.startswith(sig):
                return True
        return False
    except Exception:
        return False


def split_image_text_types(raw_docs_from_docstore):
    """
    Split base64-encoded images and texts from raw content fetched from docstore.
    """
    b64_images = []
    texts = []
    for item in raw_docs_from_docstore:
        if item is None:
            continue

        current_content_str = ""
        if isinstance(item, bytes):
            current_content_str = item.decode('utf-8', errors='ignore')
        elif isinstance(item, str):
            current_content_str = item

        if looks_like_base64(current_content_str) and is_image_data(current_content_str):
            b64_images.append(current_content_str)
        else:
            texts.append(current_content_str)
    return {"images": b64_images, "texts": texts}

# Assuming redis_store and retriever_multi_vector are initialized
query = "what is transformer"
summary_docs = retriever_multi_vector.invoke(query) # These are Document objects with summaries
doc_ids = [doc.metadata["doc_id"] for doc in summary_docs]
raw_contents = redis_store.mget(doc_ids) # This retrieves the raw content (base64 string or text) as bytes

# Now, split the raw_contents into images and texts
split_results = split_image_text_types(raw_contents)

# Access the images from the split_results
image_docs = split_results['images']

# Check if there are any images to display
if image_docs:
    plt_img_base64(image_docs[0])
else:
    print("No images found in the retrieved context for this query.")

# Check retrieval
query = "what is BERT"
docs = retriever_multi_vector.invoke(query)

# We get 3 docs
len(docs)

docs

query = "what is BERT.Explain it in detail"
summary_docs_for_test = retriever_multi_vector.invoke(query)
doc_ids_for_test = [doc.metadata["doc_id"] for doc in summary_docs_for_test]
raw_contents_for_test = redis_store.mget(doc_ids_for_test)

# Assuming docs[2] was intended to be an image from the raw content
# Check if raw_contents_for_test[2] exists and is an image
if len(raw_contents_for_test) > 2 and raw_contents_for_test[2] is not None:
    test_item = raw_contents_for_test[2]
    if isinstance(test_item, bytes):
        test_item = test_item.decode('utf-8', errors='ignore')
    print(is_image_data(test_item))
else:
    print("Raw content at index 2 is not available for testing.")

query = "Tell me detailed statistics of the top 5 years with largest wildfire acres burned"
summary_docs_for_test = retriever_multi_vector.invoke(query)
doc_ids_for_test = [doc.metadata["doc_id"] for doc in summary_docs_for_test]
raw_contents_for_test = redis_store.mget(doc_ids_for_test)

# Assuming docs[2] was intended to be an image from the raw content
if len(raw_contents_for_test) > 2 and raw_contents_for_test[2] is not None:
    test_item = raw_contents_for_test[2]
    if isinstance(test_item, bytes):
        test_item = test_item.decode('utf-8', errors='ignore')
    print(is_image_data(test_item))
else:
    print("Raw content at index 2 is not available for testing.")

query = "What is BERT"
summary_docs_for_split = retriever_multi_vector.invoke(query)
doc_ids_for_split = [doc.metadata["doc_id"] for doc in summary_docs_for_split]
raw_contents_for_split = redis_store.mget(doc_ids_for_split)

r = split_image_text_types(raw_contents_for_split)
r

# Assuming 'r' from the previous cell is correctly populated with split_image_text_types(raw_contents)
if r and r['images']:
    plt_img_base64(r['images'][0])
else:
    print("No images found in 'r' for display.")

"""## Multimodal RAG

### Build End-to-End Multimodal RAG Pipeline

Now let's connect our retriever, prompt instructions and build a multimodal RAG chain
output of the chain(when using chain.invke()) will give answer along with from which part it took the answer
"""

from operator import itemgetter
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.messages import HumanMessage

def multimodal_prompt_function(data_dict):
    """
    Create a multimodal prompt with both text and image context.

    This function formats the provided context from `data_dict`, which contains
    text, tables, and base64-encoded images. It joins the text (with table) portions
    and prepares the image(s) in a base64-encoded format to be included in a message.

    The formatted text and images (context) along with the user question are used to
    construct a prompt for GPT-4o
    """
    formatted_texts = "\n".join(data_dict["context"]["texts"])
    messages = []

    # Adding image(s) to the messages if present
    if data_dict["context"]["images"]:
        for image in data_dict["context"]["images"]:
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image}"},
            }
            messages.append(image_message)

    # Adding the text and tables for analysis
    text_message = {
        "type": "text",
        "text": (
            f"""You are an analyst tasked with understanding detailed information and trends
                from text documents, data tables, and charts and graphs in images.
                You will be given context information below which will be a mix of text, tables,
                and images usually of charts or graphs.
                Use this information to provide answers related to the user question.
                Analyze all the context information including tables, text and images to generate the answer.
                Do not make up answers, use the provided context documents below
                and answer the question to the best of your ability.

                User question:
                {data_dict['question']}

                Context documents:
                {formatted_texts}

                Answer:
            """
        ),
    }
    messages.append(text_message)
    return [HumanMessage(content=messages)]


# Create RAG chain
multimodal_rag = (
        {
            "context": itemgetter('context'),
            "question": itemgetter('input'),
        }
            |
        RunnableLambda(multimodal_prompt_function)
            |
        chatgpt
            |
        StrOutputParser()
)

# Pass input query to retriever and get context document elements
retrieve_docs = (itemgetter('input')
                    |
                retriever_multi_vector
                    |
                RunnableLambda(split_image_text_types))

# Below, we chain `.assign` calls. This takes a dict and successively
# adds keys-- "context" and "answer"-- where the value for each key
# is determined by a Runnable (function or chain executing at runtime).
# This helps in also having the retrieved context along with the answer generated by GPT-4o
multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)
                                               .assign(answer=multimodal_rag)
)

# Run RAG chain
query = "What is Attention"
response = multimodal_rag_w_sources.invoke({'input': query})
response

#just to make response presentable
def multimodal_rag_qa(query):
    response = multimodal_rag_w_sources.invoke({'input': query})
    print('=='*50)
    print('Answer:')
    display(Markdown(response['answer']))
    print('--'*50)
    print('Sources:')
    text_sources = response['context']['texts']
    img_sources = response['context']['images']
    for text in text_sources:
        display(Markdown(text))
        print()
    for img in img_sources:
        plt_img_base64(img)
        print()
    print('=='*50)

query = "Explain what is  Attention"
multimodal_rag_qa(query)